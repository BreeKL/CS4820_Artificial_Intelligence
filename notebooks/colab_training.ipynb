{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1d3uaxrFiV_J-8UXSOiUiO-jgwOycsaj_","authorship_tag":"ABX9TyMlR+78UE6Otgrdvq+9OSXU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"1wwPMALxHVry","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762916743418,"user_tz":420,"elapsed":10118,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}},"outputId":"47ffa727-c647-4443-f062-eafb4047f4d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Checking files...\n","Files found: ['configs', 'small_preprocessed_data.tar.gz', 'optuna_results_small_dataset', 'src_code.tar.gz', 'optuna_results_small_dataset_2', 'preprocessed_data.tar.gz', 'optuna_results_large_dataset', 'data', 'src', 'checkpoints', 'results']\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Verify files exist\n","import os\n","print(\"Checking files...\")\n","project_dir = '/content/drive/MyDrive/AI_project'\n","files = os.listdir(project_dir)\n","print(f\"Files found: {files}\")\n"]},{"cell_type":"code","source":["# Extracting preprocessed data to short term memory\n","# !tar -xzf /content/drive/MyDrive/AI_project/preprocessed_data.tar.gz -C /content/\n","# !mkdir -p data/processed\n","# !mv *.npz dataset_metadata.json data/processed/\n","# !mv *.npz label_info.json data/processed/\n","\n","# Extracting preprocessed data to drive memory\n","!tar -xzf /content/drive/MyDrive/AI_project/preprocessed_data.tar.gz -C /content/drive/MyDrive/AI_project/\n","!mkdir -p /content/drive/MyDrive/AI_project/data/processed\n","!mv /content/drive/MyDrive/AI_project/*.npz /content/drive/MyDrive/AI_project/dataset_metadata.json /content/drive/MyDrive/AI_project/data/processed/\n","\n","# Extract source code to Drive src folder\n","!tar -xzf /content/drive/MyDrive/AI_project/src_code.tar.gz -C /content/drive/MyDrive/AI_project/\n","\n","# Verify extractiondrwxr-xr-x 1 roodrwxr-xr-x 1 roodrwxr-xr-x 1 roodrwxr-xr-x 1 root root 4096 Nov 12 01:50 ..t root 4096 Nov 12 01:50 ..t root 4096 Nov 12 01:50 ..t root 4096 Nov 12 01:50 ..\n","!ls -la /content/drive/MyDrive/AI_project/data/processed/\n","!ls -la /content/drive/MyDrive/AI_project/src/\n","!ls -la /content/drive/MyDrive/AI_project/configs/\n"],"metadata":{"id":"VWGcxc8MO9L-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762916749536,"user_tz":420,"elapsed":6115,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}},"outputId":"473f2801-8332-4a99-a4eb-d55dfe7da208"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["total 101453\n","-rw------- 1 root root      363 Nov  9 23:11 dataset_metadata.json\n","-rw------- 1 root root 15583614 Nov  9 23:11 test_data.npz\n","-rw------- 1 root root 72719814 Nov  9 23:11 train_data.npz\n","-rw------- 1 root root 15583614 Nov  9 23:11 val_data.npz\n","total 148\n","-rw------- 1 root root 15528 Nov  4 23:05 evaluate.py\n","-rw------- 1 root root 16373 Nov  9 23:11 load_manifest_data.py\n","-rw------- 1 root root 10117 Nov  4 23:05 main.py\n","-rw------- 1 root root 10883 Nov  4 23:05 model.py\n","-rw------- 1 root root 13607 Nov 10 01:05 optuna_tuning_CPU.py\n","-rw------- 1 root root 15414 Nov 10 03:28 optuna_tuning_large_GPU.py\n","-rw------- 1 root root 15878 Nov 10 01:52 optuna_tuning_small_GPU.py\n","-rw------- 1 root root 14032 Nov 11 04:35 preprocessing.py\n","drwx------ 2 root root  4096 Nov 11 04:37 __pycache__\n","-rw------- 1 root root   734 Nov 11 04:18 test_preprocessing.py\n","-rw------- 1 root root 15921 Nov  4 23:05 train.py\n","-rw------- 1 root root 14960 Nov  4 23:05 utils.py\n","total 3\n","-rw------- 1 root root 2458 Nov  4 23:05 config.yaml\n"]}]},{"cell_type":"code","source":["# Install required packages\n","!pip install optuna pyyaml tqdm scikit-learn matplotlib astropy\n","\n","# Check PyTorch and GPU\n","import torch\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"],"metadata":{"id":"BBK-hE8KPL9J","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1762916760959,"user_tz":420,"elapsed":11399,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}},"outputId":"6685620b-4952-4111-dad0-441f21776bfe"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: astropy in /usr/local/lib/python3.12/dist-packages (7.1.1)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.12/dist-packages (from astropy) (2.0.1.5)\n","Requirement already satisfied: astropy-iers-data>=0.2025.9.29.0.35.48 in /usr/local/lib/python3.12/dist-packages (from astropy) (0.2025.11.3.0.38.37)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n","Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, optuna\n","Successfully installed colorlog-6.10.1 optuna-4.6.0\n","PyTorch version: 2.8.0+cu126\n","CUDA available: True\n","GPU: Tesla T4\n","GPU Memory: 15.83 GB\n"]}]},{"cell_type":"code","source":["# Load and verify data\n","import numpy as np\n","\n","train_data = np.load('/content/drive/MyDrive/AI_project/data/processed/train_data.npz')\n","val_data = np.load('/content/drive/MyDrive/AI_project/data/processed/val_data.npz')\n","test_data = np.load('/content/drive/MyDrive/AI_project/data/processed/test_data.npz')\n","\n","print(\"Dataset loaded successfully!\")\n","print(f\"Train: {train_data['flux'].shape}\")\n","print(f\"Val: {val_data['flux'].shape}\")\n","print(f\"Test: {test_data['flux'].shape}\")\n","\n","# Check class distribution\n","unique, counts = np.unique(train_data['labels'], return_counts=True)\n","print(f\"\\nTrain class distribution:\")\n","for cls, count in zip(unique, counts):\n","    print(f\"  Class {cls}: {count} ({100*count/len(train_data['labels']):.1f}%)\")\n"],"metadata":{"id":"62j67GkCPQbg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762916905485,"user_tz":420,"elapsed":284,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}},"outputId":"afc1c998-bb17-4a4a-c890-52d9bc5f507f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset loaded successfully!\n","Train: (2100, 4320)\n","Val: (450, 4320)\n","Test: (450, 4320)\n","\n","Train class distribution:\n","  Class 0: 1045 (49.8%)\n","  Class 1: 1055 (50.2%)\n"]}]},{"cell_type":"code","source":["# Run Optuna tuning\n","# import sys\n","# sys.path.append('/content/src')\n","\n","# Import and run for small data set on GPU\n","# !python src/optuna_tuning_small_GPU.py\n","\n","# Import and run large data set on GPU\n","# !python src/optuna_tuning_large_GPU.py"],"metadata":{"id":"cUC_JrLBPVJG","collapsed":true,"executionInfo":{"status":"ok","timestamp":1762916932098,"user_tz":420,"elapsed":20,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# After training completes, download results\n","# from google.colab import files\n","\n","# Download optimized config\n","# files.download('optuna_results/optimized_config.yaml')\n","\n","# Download best model\n","# files.download('optuna_results/optimization_results.json')\n","\n","\n","\n","# Download trained model (if using main.py)\n","# files.download('checkpoints/best_model.pth')\n"],"metadata":{"id":"BecHil8jfCVl","executionInfo":{"status":"ok","timestamp":1762916932125,"user_tz":420,"elapsed":24,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7119a65","executionInfo":{"status":"ok","timestamp":1762916932145,"user_tz":420,"elapsed":16,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}}},"source":["# import json\n","# import os\n","\n","# results_dir = '/content/drive/MyDrive/AI_project/optuna_results'\n","# results_file = os.path.join(results_dir, 'optimization_results.json')\n","\n","# try:\n","#     with open(results_file, 'r') as f:\n","#         previous_optimization_results = json.load(f)\n","\n","#     print(f\"Optuna Optimization Results from {results_dir}:\")\n","#     print(json.dumps(previous_optimization_results, indent=4))\n","\n","# except FileNotFoundError:\n","#     print(f\"Error: {results_file} not found. Please ensure the directory and file exist in your Google Drive.\")\n","# except json.JSONDecodeError:\n","#     print(f\"Error: Could not decode JSON from {results_file}\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# After training completes, download results\n","# from google.colab import files\n","\n","# # Download optimized config\n","# files.download('optuna_results/optimized_config.yaml')\n","\n","# # Download best model\n","# files.download('optuna_results/optimization_results.json')\n","\n","\n","\n","\n","# Download trained model (if using main.py)\n","# files.download('checkpoints/best_model.pth')\n"],"metadata":{"id":"IgmRlqfzPiN4","collapsed":true,"executionInfo":{"status":"ok","timestamp":1762916932161,"user_tz":420,"elapsed":13,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Copy results back to Drive for permanent storage\n","# !cp -r optuna_results /content/drive/MyDrive/AI_project/\n","# !cp -r checkpoints /content/drive/MyDrive/AI_project/ 2>/dev/null || true\n","\n","# print(\"✓ Results saved to Google Drive!\")\n"],"metadata":{"id":"rd4rMsp0Pmpj","executionInfo":{"status":"ok","timestamp":1762916932167,"user_tz":420,"elapsed":3,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Google Colab Training Setup\n","# ============================================================================\n","\n","import os\n","import sys\n","import torch\n","\n","# Set paths - adjust these to match your Drive structure\n","PROJECT_ROOT = \"/content/drive/MyDrive/AI_project\"\n","CODE_DIR = os.path.join(PROJECT_ROOT, \"src\")\n","CONFIG_PATH = os.path.join(PROJECT_ROOT, \"optuna_results_large_dataset/optimized_config.yaml\")\n","\n","# Add code directory to Python path\n","sys.path.insert(0, CODE_DIR)\n","\n","# Change to project directory\n","os.chdir(PROJECT_ROOT)\n","print(f\"Working directory: {os.getcwd()}\")\n","\n","# ============================================================================\n","# Colab-Optimized Configuration Overrides\n","# ============================================================================\n","\n","from utils import load_config, save_config\n","import yaml\n","\n","# Load your config\n","config = load_config(CONFIG_PATH)\n","\n","# Apply Colab-specific optimizations\n","print(\"\\nApplying Colab optimizations...\")\n","\n","# DataLoader settings for Colab\n","config['training']['num_workers'] = 2  # Colab works best with 1-2 workers\n","config['training']['use_amp'] = True\n","config['paths']['checkpoint_dir'] = os.path.join(PROJECT_ROOT, \"checkpoints\")\n","config['augmentation']['augmentation_factor'] = 2\n","\n","\n","# Adjust batch size if needed (based on GPU memory)\n","# config['training']['batch_size'] = 32  # Uncomment to override\n","\n","# Set device explicitly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Monitor memory usage during augmentation\n","import psutil\n","print(f\"\\nRAM Usage before training: {psutil.virtual_memory().percent}%\")\n","print(f\"Available RAM: {psutil.virtual_memory().available / 1e9:.2f} GB\")\n","\n","# ============================================================================\n","# Run Training\n","# ============================================================================\n","\n","# Import your training script components\n","from preprocessing import LightCurvePreprocessor, DataAugmenter\n","from model import create_model\n","from train import Trainer, LightCurveDataset, collate_fn\n","from utils import set_seed, count_parameters\n","\n","# Import the main training functions\n","# Since we can't use argparse in notebook, we'll call functions directly\n","import importlib.util\n","\n","# Load the main training module\n","spec = importlib.util.spec_from_file_location(\"main_train\",\n","                                               os.path.join(CODE_DIR, \"main.py\"))\n","main_module = importlib.util.module_from_spec(spec)\n","spec.loader.exec_module(main_module)\n","\n","# Call prepare_data and main training loop\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Starting Training Pipeline\")\n","print(\"=\" * 60)\n","\n","# Prepare data\n","train_loader, val_loader, test_loader = main_module.prepare_data(config)\n","\n","# Create model and move to GPU\n","print(\"\\nCreating model...\")\n","model = create_model(config['model'])\n","model = model.to(device)  # Move model to GPU\n","\n","# Print model info\n","n_params = count_parameters(model)\n","print(f\"Trainable parameters: {n_params['trainable']:,}\")\n","print(f\"Total parameters: {n_params['total']:,}\")\n","print(f\"Non-trainable parameters: {n_params['non_trainable']:,}\")\n","\n","print(f\"Model size (trainable, fp32): ~{n_params['trainable'] * 4 / 1e6:.2f} MB\")\n","\n","# Check GPU memory after model loading\n","if torch.cuda.is_available():\n","    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n","    print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n","\n","# Create trainer\n","print(\"\\nInitializing trainer...\")\n","trainer = Trainer(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    config=config['training']\n",")\n","\n","# Optional: Resume from checkpoint\n","# CHECKPOINT_PATH = \"/content/drive/MyDrive/YourProjectFolder/checkpoints/checkpoint_epoch_10.pth\"\n","# trainer.load_checkpoint(CHECKPOINT_PATH)\n","\n","# Train the model\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Starting training...\")\n","print(\"=\" * 60)\n","\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","\n","try:\n","    trainer.train(num_epochs=config['training']['num_epochs'])\n","    print(\"\\nTraining completed successfully!\")\n","\n","except RuntimeError as e:\n","    if \"out of memory\" in str(e):\n","        print(\"\\n\" + \"!\" * 60)\n","        print(\"GPU OUT OF MEMORY ERROR\")\n","        print(\"!\" * 60)\n","        print(\"Try reducing batch_size or model size\")\n","        print(f\"Current batch_size: {config['training']['batch_size']}\")\n","        print(f\"Current d_model: {config['model']['d_model']}\")\n","\n","        # Clear GPU cache\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","    raise e\n","\n","# Check final memory usage\n","print(f\"\\nFinal RAM Usage: {psutil.virtual_memory().percent}%\")\n","if torch.cuda.is_available():\n","    print(f\"Final GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n","\n","# ============================================================================\n","# Evaluate on Test Set\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"Evaluating on test set...\")\n","print(\"=\" * 60)\n","\n","from evaluate import ModelEvaluator\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","\n","# Load best model\n","best_model_path = Path(config['paths']['checkpoint_dir']) / 'best_model.pth'\n","checkpoint = torch.load(best_model_path, weights_only=False)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model = model.to(device)\n","\n","evaluator = ModelEvaluator(model)\n","metrics = evaluator.evaluate(\n","    test_loader,\n","    class_names=config.get('classes', None),\n","    save_dir=config['paths']['results_dir']\n",")\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"FINAL RESULTS\")\n","print(\"=\" * 60)\n","print(f\"Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n","print(f\"Test accuracy: {metrics['accuracy']:.4f}\")\n","print(f\"Test precision: {metrics['precision']:.4f}\")\n","print(f\"Test recall: {metrics['recall']:.4f}\")\n","print(f\"Test F1: {metrics['f1_score']:.4f}\")\n","auc_score = metrics.get('auc', metrics.get('roc_auc', 0.0)) * 100\n","print(f\"Test AUC: {auc_score:.4f}\")\n","print(f\"\\nResults saved to: {config['paths']['results_dir']}\")\n","\n","# ============================================================================\n","# Visualize Performance Metrics\n","# ============================================================================\n","\n","# Convert metrics to percentages\n","metric_names = ['AUC', 'F1 Score', 'Recall', 'Precision', 'Accuracy']\n","metric_values = [\n","    auc_score,\n","    metrics['f1_score'],\n","    metrics['recall'] * 100,\n","    metrics['precision'] * 100,\n","    metrics['accuracy'] * 100\n","]\n","\n","# Sort metrics for better visualization\n","# sorted_pairs = sorted(zip(metric_values, metric_names), reverse=True)\n","# sorted_values, sorted_names = zip(*sorted_pairs)\n","\n","# Create horizontal bar chart\n","plt.figure(figsize=(10, 5))\n","bars = plt.barh(metric_names, metric_values, color='blue', edgecolor='navy', alpha=0.5)\n","plt.xlabel('Score (%)', fontsize=12)\n","plt.title('Model Performance Metrics', fontsize=14, fontweight='bold')\n","plt.xlim(0, 100)\n","\n","# Annotate bars with score values\n","for bar, score in zip(bars, metric_values):\n","    plt.text(score + 1, bar.get_y() + bar.get_height()/2,\n","             f'{score:.1f}%', va='center', fontsize=10)\n","\n","# plt.gca().invert_yaxis()  # Highest score at the top\n","plt.grid(axis='x', alpha=0.3)\n","plt.tight_layout()\n","\n","# Save the figure\n","metrics_plot_path = Path(config['paths']['results_dir']) / 'performance_metrics.png'\n","plt.savefig(metrics_plot_path, dpi=300, bbox_inches='tight')\n","print(f\"Performance metrics plot saved to: {metrics_plot_path}\")\n","\n","# Display in notebook\n","plt.show()\n","\n","# Optional: Download results to local machine\n","from google.colab import files\n","files.download(os.path.join(config['paths']['results_dir'], 'confusion_matrix.png'))\n","#files.download(os.path.join(config['paths']['checkpoint_dir'], 'best_model.pth'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"95LgCVUBCuNZ","executionInfo":{"status":"error","timestamp":1762917473688,"user_tz":420,"elapsed":452501,"user":{"displayName":"Brianne Leatherman","userId":"13676261800739890295"}},"outputId":"a2e7b076-f160-4549-e225-16755340145e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Working directory: /content/drive/MyDrive/AI_project\n","\n","Applying Colab optimizations...\n","Using device: cuda\n","\n","RAM Usage before training: 13.8%\n","Available RAM: 11.73 GB\n","\n","============================================================\n","Starting Training Pipeline\n","============================================================\n","Preparing datasets...\n","Loading preprocessed data...\n","  Train: 2100 samples\n","  Val:   450 samples\n","  Test:  450 samples\n","\n","Original class distribution:\n","  Train: class_0=1045 (49.8%) class_1=1055 (50.2%)\n","  Val: class_0=241 (53.6%) class_1=209 (46.4%)\n","  Test: class_0=214 (47.6%) class_1=236 (52.4%)\n","Applying data augmentation...\n","\n","Applying additional augmentation (multiplier: 2)...\n","  Augmentation complete!                              \n","  Original size: 2100\n","  Augmented size: 4200\n","  Increase: 2.0x\n","\n","Augmented class distribution:\n","  Train: class_0=2090 (49.8%) class_1=2110 (50.2%)\n","Training samples: 4200\n","Validation samples: 450\n","Test samples: 450\n","\n","Creating model...\n","Trainable parameters: 108,706\n","Total parameters: 108,706\n","Non-trainable parameters: 0\n","Model size (trainable, fp32): ~0.43 MB\n","GPU Memory allocated: 0.00 GB\n","GPU Memory cached: 0.00 GB\n","\n","Initializing trainer...\n","\n","Class distribution:\n","  Class 0: 2090 samples (49.8%), weight: 1.005\n","  Class 1: 2110 samples (50.2%), weight: 0.995\n","Class weights: tensor([1.0048, 0.9952])\n","Using label smoothing: 0.011383962924475872\n","Using warmup scheduler: 5 warmup epochs\n","Using gradient clipping: 1.951167441590346\n","\n","============================================================\n","Starting training...\n","============================================================\n","\n","======================================================================\n","Training Configuration\n","======================================================================\n","Device: cuda\n","Model parameters: 108,706\n","Trainable parameters: 108,706\n","Learning rate: 0.0004765943762197057\n","Batch size: 16\n","Weight decay: 9.915381986403726e-05\n","Dropout: 0.18449563826785\n","======================================================================\n","\n","\n","Epoch 1/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6703 | Train Acc: 58.17%\n","Val Loss:   0.6338 | Val Acc:   60.22%\n","LR:         9.53e-05\n","✓ Validation improved by inf\n","✓ Saved best model (val_loss: 0.6338)\n","\n","Epoch 2/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6320 | Train Acc: 63.50%\n","Val Loss:   0.6339 | Val Acc:   60.44%\n","LR:         1.91e-04\n","✗ No improvement for 1 epochs\n","\n","Epoch 3/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6247 | Train Acc: 63.67%\n","Val Loss:   0.6237 | Val Acc:   61.33%\n","LR:         2.86e-04\n","✓ Validation improved by 0.0101\n","✓ Saved best model (val_loss: 0.6237)\n","\n","Epoch 4/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6167 | Train Acc: 64.95%\n","Val Loss:   0.6334 | Val Acc:   59.78%\n","LR:         3.81e-04\n","✗ No improvement for 1 epochs\n","\n","Epoch 5/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6217 | Train Acc: 64.67%\n","Val Loss:   0.6382 | Val Acc:   60.44%\n","LR:         4.77e-04\n","✗ No improvement for 2 epochs\n","\n","Epoch 6/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6198 | Train Acc: 63.64%\n","Val Loss:   0.6259 | Val Acc:   59.56%\n","LR:         4.76e-04\n","✗ No improvement for 3 epochs\n","\n","Epoch 7/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6143 | Train Acc: 64.48%\n","Val Loss:   0.6208 | Val Acc:   61.33%\n","LR:         4.76e-04\n","✓ Validation improved by 0.0029\n","✓ Saved best model (val_loss: 0.6208)\n","\n","Epoch 8/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6086 | Train Acc: 64.71%\n","Val Loss:   0.6325 | Val Acc:   57.78%\n","LR:         4.75e-04\n","✗ No improvement for 1 epochs\n","\n","Epoch 9/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6078 | Train Acc: 65.29%\n","Val Loss:   0.6465 | Val Acc:   60.00%\n","LR:         4.75e-04\n","✗ No improvement for 2 epochs\n","\n","Epoch 10/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6108 | Train Acc: 64.12%\n","Val Loss:   0.6315 | Val Acc:   61.33%\n","LR:         4.73e-04\n","✗ No improvement for 3 epochs\n","\n","Epoch 11/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.6004 | Train Acc: 65.07%\n","Val Loss:   0.6447 | Val Acc:   60.22%\n","LR:         4.72e-04\n","✗ No improvement for 4 epochs\n","\n","Epoch 12/100\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Train Loss: 0.5994 | Train Acc: 65.60%\n","Val Loss:   0.6444 | Val Acc:   60.44%\n","LR:         4.70e-04\n","✗ No improvement for 5 epochs\n","\n","======================================================================\n","Early stopping triggered after 12 epochs\n","======================================================================\n","\n","======================================================================\n","Training completed in 7.18 minutes\n","Best validation loss: 0.6208\n","======================================================================\n","\n","\n","Training completed successfully!\n","\n","Final RAM Usage: 20.2%\n","Final GPU Memory allocated: 0.02 GB\n","\n","============================================================\n","Evaluating on test set...\n","============================================================\n","\n","======================================================================\n","STARTING EVALUATION\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Predicting: 100%|██████████| 29/29 [00:01<00:00, 21.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluation Metrics:\n","============================================================\n","accuracy            : 0.6333\n","precision           : 0.6437\n","recall              : 0.6737\n","f1_score            : 0.6584\n","roc_auc             : 0.6766\n","\n","Classification Report:\n","============================================================\n","              precision    recall  f1-score   support\n","\n"," Non-Transit     0.6207    0.5888    0.6043       214\n","     Transit     0.6437    0.6737    0.6584       236\n","\n","    accuracy                         0.6333       450\n","   macro avg     0.6322    0.6313    0.6314       450\n","weighted avg     0.6328    0.6333    0.6327       450\n","\n","\n","Saving visualizations to results\n","Saved confusion matrix to results/confusion_matrix.png\n","Saved ROC curve to results/roc_curve.png\n","\n","======================================================================\n","EVALUATION COMPLETE\n","======================================================================\n","\n","============================================================\n","FINAL RESULTS\n","============================================================\n","Best validation loss: 0.6208\n","Test accuracy: 0.6333\n","Test precision: 0.6437\n","Test recall: 0.6737\n","Test F1: 0.6584\n","Test AUC: 67.6560\n","\n","Results saved to: results\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'f1_score' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2367036028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m metric_values = [\n\u001b[1;32m    186\u001b[0m     \u001b[0mauc_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"]}]},{"cell_type":"code","source":["# Optional: Download results to local machine\n","# from google.colab import files\n","# files.download(os.path.join(config['paths']['results_dir'], 'confusion_matrix.png'))"],"metadata":{"id":"q6WypHPEFE0G"},"execution_count":null,"outputs":[]}]}